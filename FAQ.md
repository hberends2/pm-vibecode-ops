# Frequently Asked Questions

Questions from Product Managers using AI-powered development workflows.

## Table of Contents

**Setup & Installation**
- [What if I've never used a terminal before?](#what-if-ive-never-used-a-terminal-before)
- [How do I install the workflow commands?](#how-do-i-install-the-workflow-commands)
- [What MCP servers do I need?](#what-mcp-servers-do-i-need)
- [Global vs local installation - which should I use?](#global-vs-local-installation---which-should-i-use)

**Getting Started**
- [Do I need to know how to code?](#do-i-need-to-know-how-to-code)
- [What tools do I need?](#what-tools-do-i-need)
- [How much does this cost?](#how-much-does-this-cost)
- [Can I use this with my existing codebase?](#can-i-use-this-with-my-existing-codebase)

**Workflow Modes**
- [Which workflow mode should I use?](#which-workflow-mode-should-i-use)
- [What's the difference between Simple and Worktree mode?](#whats-the-difference-between-simple-and-worktree-mode)
- [Can I switch between modes?](#can-i-switch-between-modes)

**Workflow Questions**
- [Should I run multiple commands in the same session?](#should-i-run-multiple-commands-in-the-same-session)
- [How long does each phase take?](#how-long-does-each-phase-take)
- [Can I skip steps to move faster?](#can-i-skip-steps-to-move-faster)
- [What if my team doesn't use Linear?](#what-if-my-team-doesnt-use-linear)
- [Do I need to run the full workflow every time?](#do-i-need-to-run-the-full-workflow-every-time)

**Quality & Control**
- [How do I know the code is good quality?](#how-do-i-know-the-code-is-good-quality)
- [What if the AI makes a mistake?](#what-if-the-ai-makes-a-mistake)
- [Can this replace my engineering team?](#can-this-replace-my-engineering-team)
- [How do I verify AI work without reading code?](#how-do-i-verify-ai-work-without-reading-code)

**PRDs & Requirements**
- [How detailed should my PRD be?](#how-detailed-should-my-prd-be)
- [Can I use user stories instead of a PRD?](#can-i-use-user-stories-instead-of-a-prd)
- [What if requirements change mid-development?](#what-if-requirements-change-mid-development)
- [How do I write good acceptance criteria?](#how-do-i-write-good-acceptance-criteria)

**Security & Compliance**
- [Is AI-generated code secure?](#is-ai-generated-code-secure)
- [Can I use this for HIPAA/SOC2 compliance?](#can-i-use-this-for-hipaasoc2-compliance)
- [Who owns the code generated by AI?](#who-owns-the-code-generated-by-ai)
- [What about secrets and API keys?](#what-about-secrets-and-api-keys)

**Team & Organization**
- [How do I get my engineers to trust this?](#how-do-i-get-my-engineers-to-trust-this)
- [Can multiple PMs use this on the same codebase?](#can-multiple-pms-use-this-on-the-same-codebase)
- [What if I have offshore/distributed teams?](#what-if-i-have-offshoredistributed-teams)
- [How do I train my team on this workflow?](#how-do-i-train-my-team-on-this-workflow)

**Troubleshooting**
- [The AI created duplicate epics. How do I fix this?](#the-ai-created-duplicate-epics-how-do-i-fix-this)
- [Tests are failing. What do I do?](#tests-are-failing-what-do-i-do)
- [Security review found critical issues. Now what?](#security-review-found-critical-issues-now-what)
- [The implementation doesn't match my PRD. Help!](#the-implementation-doesnt-match-my-prd-help)

---

## Setup & Installation

### What if I've never used a terminal before?

**Don't worry!** We have a complete guide for beginners.

See [Setup Guide](docs/SETUP_GUIDE.md) which covers:
- What a terminal is and how to open it
- Basic commands you'll need
- Step-by-step installation of everything
- Common errors and how to fix them

**Time needed**: About 45-60 minutes for complete setup

---

### How do I install the workflow commands?

The workflow supports both **Claude Code** (slash commands + agents) and **OpenAI Codex** (platform-agnostic prompts).

**For Claude Code (global installation)**:
```bash
# Clone repository
git clone https://github.com/YOUR_ORG/pm-vibecode-ops.git
cd pm-vibecode-ops

# Create directories
mkdir -p ~/.claude/commands ~/.claude/agents

# For Simple Mode (recommended):
cp claude/commands/*.md ~/.claude/commands/
cp claude/agents/*.md ~/.claude/agents/

# OR for Worktree Mode (advanced):
cp claude/commands-worktrees/*.md ~/.claude/commands/
cp claude/agents/*.md ~/.claude/agents/
```

**For OpenAI Codex**:
```bash
# Clone repository
git clone https://github.com/YOUR_ORG/pm-vibecode-ops.git

# Use prompts directly from:
ls pm-vibecode-ops/codex/prompts/

# Or copy to Codex directory:
mkdir -p ~/.codex/prompts
cp pm-vibecode-ops/codex/prompts/*.md ~/.codex/prompts/
```

**Verify installation**:
```bash
# For Claude Code
ls ~/.claude/commands/
# Should show: adaptation.md, codereview.md, discovery.md, etc.

# For Codex
ls ~/.codex/prompts/  # or pm-vibecode-ops/codex/prompts/
```

See [Setup Guide](docs/SETUP_GUIDE.md) for detailed instructions.

---

### What MCP servers do I need?

**Required**:
- **Linear MCP** - For creating and managing tickets

**Recommended**:
- **Perplexity MCP** - For web research during discovery
- **Sequential Thinking MCP** - For complex reasoning tasks

**Optional**:
- **Playwright MCP** - For browser-based testing

See [MCP Setup Guide](docs/MCP_SETUP.md) for installation instructions.

---

### Global vs local installation - which should I use?

| Installation | Location | Best For |
|--------------|----------|----------|
| **Global** | `~/.claude/` or `~/.codex/` | Using across multiple projects |
| **Local** | `project/.claude/` | Project-specific customization |

**Our recommendation**: Start with global installation. It's simpler and works for most use cases.

**When to use local**:
- You want different command versions per project
- Your team has project-specific customizations
- You're committing the workflow to the project repo
- You need both Simple and Worktree modes in different projects

**Note for Codex users**: Codex prompts can be referenced directly from the cloned repository without copying, making them effectively "global" by default.

---

## Getting Started

### Do I need to know how to code?

**Short answer**: No.

**Longer answer**: You need to understand:
- What your users need (product thinking)
- How to write clear requirements
- How to review outcomes against requirements

You do **NOT** need to:
- Write code syntax
- Understand frameworks or libraries
- Debug technical errors
- Configure development environments

**What helps**:
- Understanding basic tech concepts (databases, APIs, users vs. systems)
- Being comfortable with uncertainty
- Trusting the process and quality gates

---

### What tools do I need?

**Required**:
1. **AI Coding Assistant**: Claude Code (recommended) or similar
2. **Project Management**: Linear (recommended), Jira, or GitHub Issues
3. **Version Control**: Git repository access
4. **Text Editor**: Any text editor for writing PRDs (VS Code, Notion, Google Docs)

**Nice to have**:
- **Design Tools**: Figma for UI mockups (helps with requirements)
- **Analytics**: For measuring success metrics post-launch
- **Communication**: Slack/Discord for team coordination

---

### Can I use this with my existing codebase?

**Yes!** This workflow is designed for existing codebases.

**How it works**:
1. **Service Inventory** scans your existing code
2. **Discovery** learns your patterns and conventions
3. **AI generates code** matching your existing style

**Requirements**:
- Git repository (nearly all projects have this)
- Documented or inferable patterns
- Modern tech stack (post-2015 frameworks)

**Works best with**:
- React, Vue, Angular (frontend)
- Node.js, Python, Ruby, Go (backend)
- PostgreSQL, MySQL, MongoDB (databases)
- Modern frameworks (Next.js, NestJS, Django, Rails)

**Less ideal for**:
- Legacy codebases (pre-2010)
- Proprietary frameworks
- Heavily customized architectures
- Extremely niche technologies

---

## Workflow Modes

### Which workflow mode should I use?

**Use Simple Mode** if:
- You're new to this workflow
- You work on one ticket at a time
- Your team has one PM/developer working on features
- You want the most straightforward experience

**Use Worktree Mode** if:
- You need multiple AI agents working simultaneously
- Your team works on several tickets in parallel
- You're comfortable with git worktrees
- You've already mastered Simple Mode

**Our recommendation**: Start with Simple Mode. After you've shipped 3-5 features successfully, consider Worktree Mode if you need concurrent development.

---

### What's the difference between Simple and Worktree mode?

| Aspect | Simple Mode | Worktree Mode |
|--------|-------------|---------------|
| **Concurrency** | One ticket at a time | Multiple tickets simultaneously |
| **Complexity** | Lower, easier to learn | Higher, more concepts to understand |
| **Git knowledge** | Basic branches | Worktrees (advanced) |
| **Isolation** | Shared branch space | Complete file system isolation |
| **Commands** | `claude/commands/` | `claude/commands-worktrees/` |
| **Status** | Stable, recommended | Advanced |

**How they differ technically**:

**Simple Mode** creates a feature branch for each ticket:
```
main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂
    ‚ï≤                       ‚ï±
     ‚ï≤‚îÄ‚îÄ feature/TICKET-123 ‚ï±
```

**Worktree Mode** creates isolated directories:
```
repo/
‚îú‚îÄ‚îÄ src/                    # Main repo
‚îî‚îÄ‚îÄ .worktrees/
    ‚îú‚îÄ‚îÄ TICKET-123/         # Isolated copy
    ‚îî‚îÄ‚îÄ TICKET-456/         # Another isolated copy
```

---

### Can I switch between modes?

**Yes**, you can switch modes, but with some considerations:

**Switching from Simple to Worktree**:
1. Finish any in-progress tickets first
2. Start using commands from `claude/commands-worktrees/`
3. Read the [Worktree Guide](docs/WORKTREE_GUIDE.md) first

**Switching from Worktree to Simple**:
1. Complete and merge all active worktrees
2. Run `git worktree prune` to clean up
3. Switch to commands from `claude/commands/`

**Important**: Don't mix commands from both modes on the same ticket. Pick one mode and stick with it for that ticket's entire lifecycle.

See [Worktree Migration Guide](docs/WORKTREE_MIGRATION_GUIDE.md) for detailed steps.

---

## Workflow Questions

### Should I run multiple commands in the same session?

**Short answer:** No. Run each command in a fresh, separate Claude Code session.

**Why this matters:**

Each workflow command generates substantial context‚Äîdiscovery findings, code analysis, security reports, test results, etc. When you chain multiple commands in the same session:

1. **Context overflow:** You'll hit Claude's context window limits, causing degraded performance or errors
2. **Context pollution:** Later phases see irrelevant details from earlier phases (e.g., security review doesn't need implementation discussions)
3. **Reduced effectiveness:** Each phase works best with focused, relevant context

**Best practice workflow:**

```bash
# Session 1
claude
/discovery prd.md ProjectName ./src
# Review output
exit

# Session 2 (fresh context)
claude
/epic-planning prd.md discovery-report.md
# Review output
exit

# Session 3 (fresh context)
claude
/planning EPIC-123
# Review output
exit

# Continue this pattern...
```

**Benefits you'll see:**
- Commands run faster with clean context
- More accurate results (no context confusion)
- Better recommendations (focused on current phase)
- Fewer errors or unexpected behaviors

**"But doesn't this lose my progress?"**

No! Each command creates persistent artifacts:
- Discovery creates Linear tickets with findings
- Epic planning creates epics in your project management tool
- Implementation creates code commits and PRs
- Documentation creates committed docs

Starting a fresh session just gives the AI clean context to work with‚Äîall your progress is saved in your repository and ticketing system.

**Time cost:** Starting a new session takes 5-10 seconds. The quality improvement is worth it.

---

### How long does each phase take?

**Typical timeline** (small to medium feature):

| Phase              | AI Time       | PM Time       | Total             |
| ------------------ | ------------- | ------------- | ----------------- |
| Service Inventory  | 2-5 min       | 5 min review  | 10 min            |
| Discovery          | 5-10 min      | 15 min review | 25 min            |
| Epic Planning      | 10-15 min     | 20 min review | 35 min            |
| Technical Planning | 5-10 min      | 15 min review | 25 min            |
| Implementation     | 2-4 hours     | 0 min         | 2-4 hours         |
| Testing            | 30-60 min     | 10 min review | 40-70 min         |
| Code Review        | 10-15 min     | 5 min review  | 20 min            |
| Security Review    | 15-20 min     | 10 min review | 30 min            |
| **Total**          | **4-7 hours** | **1.5 hours** | **5.5-8.5 hours** |

**For your first feature**: Add 50% time for learning.

**For complex features**: Multiply by 2-3x.

**Comparison to traditional**:
- Traditional development: Typically much longer for routine features
- This workflow: Significantly faster (50-75% time savings typical)
- Actual savings vary by feature complexity and team experience

---

### Can I skip steps to move faster?

**Short answer**: Don't skip, but you can run some in parallel.

**Never skip**:
- ‚ùå Security Review (critical vulnerabilities could ship)
- ‚ùå Service Inventory (will create duplicate code)
- ‚ùå Epic Planning approval (waste time building wrong thing)

**Can skip** (with caution):
- ‚ö†Ô∏è Discovery (if you've done it recently for this code area)
- ‚ö†Ô∏è Code Review (if it's a tiny change)

**Can run in parallel**:
- ‚úÖ Service Inventory + reading PRD
- ‚úÖ Testing + Documentation
- ‚úÖ Code Review + Security Review (for small changes)

**Real mistake**: One PM skipped service inventory to "save 10 minutes." AI rebuilt an authentication service that already existed. Took 2 days to refactor. **Lesson**: The 10-minute service inventory saves hours.

---

### What if my team doesn't use Linear?

**Linear is recommended** but not required.

**Adapting for other tools**:

**Jira**:
- Use Jira epics instead of Linear epics
- Use Jira MCP for automation (similar to Linear MCP)
- Workflow is identical, just different tool

**GitHub Issues**:
- Use GitHub Issues + Projects
- Labels replace Linear's project structure
- Milestones replace Linear's epics
- Slightly less structured, but works

**Asana/Monday/ClickUp**:
- Use their hierarchy (Projects ‚Üí Tasks)
- Manual updates (no MCP integration currently available)
- More overhead, but functional

**How to adapt**:
1. Replace Linear MCP commands with manual steps
2. Create epics/issues manually in your tool
3. Copy AI findings into your tool's comments/description
4. Rest of workflow stays the same

**Trade-off**: Manual tools require 10-15 minutes more PM time per feature.

---

### Do I need to run the full workflow every time?

**It depends on the change**:

**Full workflow** (all 9 phases):
- ‚úÖ New features
- ‚úÖ Complex changes
- ‚úÖ Changes affecting multiple areas
- ‚úÖ Anything customer-facing

**Abbreviated workflow** (skip some phases):
- Small bug fixes: Skip discovery, just implementation + testing + documentation
- Content changes: Just implementation + documentation
- Configuration changes: Implementation + security review + documentation
- Refactoring: Skip epic planning, run all quality gates including documentation

**Decision tree**:
```
Does this create new user-facing functionality?
‚îú‚îÄ Yes ‚Üí Full workflow (all 10 phases)
‚îî‚îÄ No ‚Üí Is it changing business logic?
    ‚îú‚îÄ Yes ‚Üí Implementation + Testing + Documentation + Code Review + Security Review
    ‚îî‚îÄ No ‚Üí Implementation + Testing + Documentation + Code Review + Security Review
```

**Note**: Security review is always the final gate that closes tickets.

**Rule of thumb**: If you're uncertain, run the full workflow. The time investment is modest compared to the quality benefits.

---

### What are git worktrees and why does this workflow use them?

**Short answer**: Worktrees enable multiple AI agents to work on different tickets simultaneously without conflicts.

**Longer answer**:

**What they are**:
Git worktrees create isolated working directories for each ticket. Instead of switching branches and risking conflicts, each ticket gets its own space.

```
repo/                           # Main repository
‚îú‚îÄ‚îÄ .worktrees/
‚îÇ   ‚îú‚îÄ‚îÄ TICKET-101/            # Agent working on authentication
‚îÇ   ‚îú‚îÄ‚îÄ TICKET-102/            # Agent working on payments
‚îÇ   ‚îî‚îÄ‚îÄ TICKET-103/            # Agent working on reports
```

**Why this matters**:
- **Concurrent Development**: Multiple AI agents (or PMs) can work on different tickets at the same time
- **Zero Conflicts**: Changes to TICKET-101 never affect TICKET-102's work
- **No Context Switching**: Don't need to stash, commit, or switch branches
- **Automatic Management**: The workflow handles everything - you never manually manage worktrees

**User Impact** (what you experience):
1. You run `/adaptation TICKET-123` ‚Üí Worktree created automatically
2. You run `/implementation TICKET-123` ‚Üí Works in isolated directory
3. You run `/security_review TICKET-123` ‚Üí Merges changes and cleans up

**You never need to think about worktrees.** They're managed automatically.

**See WORKTREE_GUIDE.md for technical details.**

---

### Troubleshooting: "Worktree not found" error

**Problem**: Command reports worktree doesn't exist for a ticket.

**Error message**:
```
‚ùå ERROR: Worktree not found at /path/to/repo/.worktrees/TICKET-123
```

**Solutions**:

**Solution 1: Run adaptation first** (90% of cases)
```bash
# Worktrees are created by the adaptation phase
/adaptation TICKET-123

# Then run your command
/implementation TICKET-123
```

**Solution 2: Check Linear ticket** (5% of cases)
- Open the Linear ticket
- Look in comments for "**Worktree Path**:"
- Verify the path exists: `ls -la /path/to/worktree`
- If path doesn't exist, re-run `/adaptation TICKET-123`

**Solution 3: List active worktrees** (5% of cases)
```bash
# See all active worktrees
git worktree list

# If ticket's worktree is missing, recreate it
/adaptation TICKET-123
# Choose "Remove and recreate" if prompted
```

**Solution 4: Worktree corrupted**
```bash
# Remove corrupted worktree
git worktree remove --force .worktrees/TICKET-123

# Recreate fresh worktree
/adaptation TICKET-123
```

**Common causes**:
1. Skipped adaptation phase ‚Üí Run `/adaptation` first
2. Manual deletion of worktree directory ‚Üí Re-run `/adaptation`
3. Wrong ticket ID ‚Üí Verify ticket ID matches Linear
4. Repository corruption ‚Üí Remove and recreate worktree

**Prevention**:
- Always run `/adaptation` before any other ticket-level commands
- Don't manually delete `.worktrees/` directories
- Let security review handle cleanup automatically

---

### Can multiple people work on the same codebase without conflicts?

**Yes! This is exactly what worktrees solve.**

**Scenario: Two PMs/agents working simultaneously**

```bash
# Terminal 1: PM Alice works on authentication
cd /path/to/my-app
/adaptation TICKET-101  # Creates .worktrees/TICKET-101/
/implementation TICKET-101  # Works in isolation

# Terminal 2: PM Bob works on payments (at the same time!)
cd /path/to/my-app
/adaptation TICKET-102  # Creates .worktrees/TICKET-102/
/implementation TICKET-102  # Works in isolation

# NO CONFLICTS! Each ticket has its own directory.
```

**How it works**:
- Each ticket gets dedicated `.worktrees/[ticket-id]` directory
- Changes in one worktree don't affect others
- When security review passes, changes merge to main cleanly
- Multiple AI agents can run concurrently

**Best practices for teams**:
1. **Shared service inventory**: Run weekly, share with team
2. **Communication**: Daily standup or Slack for coordination
3. **Frequent merging**: Merge to main often (avoid long-lived worktrees)
4. **Coordination**: If editing same service, coordinate timing

**Real example**:
```
Monday 9am:  PM A starts TICKET-101 (authentication)
Monday 9am:  PM B starts TICKET-102 (payments) ‚Üê concurrent!
Monday 2pm:  TICKET-101 completes, merges to main
Monday 4pm:  TICKET-102 completes, merges to main
Both features: Zero conflicts, clean merges
```

**See [Can multiple PMs use this on the same codebase?](#can-multiple-pms-use-this-on-the-same-codebase) for detailed coordination strategies.**

---

## Quality & Control

### How do I know the code is good quality?

**Built-in quality indicators**:

‚úÖ **Test Coverage**: Target is 90%+
- You'll see: "Test coverage: 94%" in reports
- Below 90% ‚Üí Ask AI to add more tests

‚úÖ **Code Review**: Automated pattern checking
- You'll see: "‚úÖ No anti-patterns detected"
- Any ‚ö†Ô∏è warnings ‚Üí Review and fix

‚úÖ **Security Review**: OWASP compliance
- You'll see: "No critical or high severity issues"
- Any üö® CRITICAL ‚Üí Must fix before merge

‚úÖ **Performance**: Benchmark tests
- You'll see: "p95 latency: 234ms"
- Exceeds targets ‚Üí Must optimize

**PM-friendly quality checks**:
1. **Does it match PRD?** Read the PR description‚Äîdoes it cover all acceptance criteria?
2. **Do tests read like user scenarios?** Test names should be understandable (e.g., "should allow user to upload photo")
3. **Are there any ‚ö†Ô∏è or üö® in reports?** Any warnings require attention
4. **Did quality gates pass?** All should show ‚úÖ

**Red flags**:
- üö© Test coverage below 80%
- üö© Multiple anti-patterns in code review
- üö© Security issues marked CRITICAL or HIGH
- üö© Performance metrics missing or failing

---

### What if the AI makes a mistake?

**AI makes mistakes** in about 5-10% of cases. The workflow is designed to catch them.

**Where mistakes are caught**:

**Code Review** (catches ~60% of issues):
- Logic errors
- Pattern violations
- Performance problems

**Testing** (catches ~30% of issues):
- Edge cases not handled
- Error conditions missed
- Integration problems

**Security Review** (catches ~8% of issues):
- Security vulnerabilities
- Data leaks
- Permission errors

**Manual Review** (catches ~2% of issues):
- Requirements misunderstood
- UX problems
- Business logic errors

**When you find a mistake**:

1. **Don't panic** - mistakes are normal and expected
2. **Describe what's wrong** in plain language (not code fixes). 
3. **Cut and paste** technical details/errors from logs, browser development console, etc., where you can
4. **AI will fix it** - usually within minutes
5. **Re-run quality gates** to verify fix

**Example mistake**:
```
PM: "The export button is visible for users who don't have permission.
This should only show for users with 'export' permission."

AI: *Reviews code, finds permission check missing*
AI: *Adds permission check to button visibility logic*
AI: *Re-runs tests*
AI: "Fixed. Added permission check. 3 new tests added for this scenario."
```

**Cost of mistakes**:
- Caught in code review: 10 minutes to fix
- Caught in testing: 15 minutes to fix
- Caught in security review: 20 minutes to fix
- Caught in production: Hours to days to fix

**This is why we don't skip quality gates.**

---

### Can this replace my engineering team?

**No.** This is not a replacement‚Äîit's an amplifier.

**What this CAN do**:
- ‚úÖ Handle straightforward CRUD features
- ‚úÖ Implement well-defined user stories
- ‚úÖ Extend existing patterns
- ‚úÖ Write tests and documentation
- ‚úÖ Free engineers for complex work

**What this CANNOT do**:
- ‚ùå Design system architecture from scratch
- ‚ùå Make complex trade-off decisions
- ‚ùå Solve novel technical problems
- ‚ùå Debug production incidents
- ‚ùå Mentor junior developers
- ‚ùå Do research or experimentation

**Best model**:

**PM + AI**: Straightforward features (70% of backlog)
- User profiles
- Data exports
- Configuration UIs
- Standard CRUD operations

**Engineers + AI**: Complex features (30% of backlog)
- New architectural components
- Performance optimization
- Complex algorithms
- Infrastructure design

**Example split** (from real company):
- Before: 3 engineers, 80% time on basic features, 20% on complex work
- After: PM handles 70% of basic features with AI, engineers focus 100% on complex work
- Result: **2.4x overall productivity increase**

**Engineering reaction**:
Most engineers LOVE this because they:
- Stop doing boring CRUD work
- Focus on interesting challenges
- See PM understand technical work better
- Ship more impactful features

---

### How do I verify AI work without reading code?

**You don't verify code‚Äîyou verify outcomes.**

**Verification checklist**:

**1. Acceptance Criteria Match** (10 minutes)
```
PR Description says:
‚úÖ Export button on all dashboards
‚úÖ CSV format with headers
‚úÖ File name format: dashboard-{name}-{date}.csv
‚úÖ Supports up to 10,000 rows
‚úÖ Progress indicator for large exports

Your PRD required all of this ‚Üí ‚úÖ VERIFIED
```

**2. Test Coverage Report** (5 minutes)
```
Coverage: 94% ‚úÖ
Tests: 47 passing, 0 failing ‚úÖ
Test scenarios include:
- Happy path: user exports 100 rows ‚úÖ
- Large dataset: user exports 10,000 rows ‚úÖ
- Error case: dataset exceeds 10,000 rows ‚úÖ
- Permission check: unauthorized user can't export ‚úÖ
```

**3. Quality Gate Results** (5 minutes)
```
Code Review: ‚úÖ Passed
Security Review: ‚úÖ No critical issues
Performance Tests: ‚úÖ Meets targets
All checks passing ‚úÖ
```

**4. Manual Testing** (15 minutes)
- Log into app
- Navigate to a dashboard
- Click export button
- Verify CSV downloads correctly
- Verify file content matches dashboard
- Try as different user roles

**Total verification time: 35 minutes** (no code reading required)

**If anything doesn't match**:
1. Open PR comments
2. Describe what's wrong
3. AI will fix and re-run quality gates
4. Verify again

---

## PRDs & Requirements

### How detailed should my PRD be?

**Sweet spot**: Detailed enough for clarity, not so detailed you're writing code.

**Too vague** ‚ùå:
```
Add reporting features
```

**Too detailed** ‚ùå:
```
Create a ReportService class with a method generateCSV() that takes a
reportId parameter, queries the reports table using Prisma, formats
the data using the csv-writer library...
```

**Just right** ‚úÖ:
```
Users should be able to export dashboard data to CSV format. The export
should include all visible columns with descriptive headers, respect
current filters, and handle datasets up to 10,000 rows with proper
progress indication.

Success criteria:
- 30% of power users export data weekly
- Export completion rate >95%
- Generation time <5 seconds for 5,000 rows
```

**Checklist for "right" level of detail**:

Must have:
- [ ] Clear problem statement (why)
- [ ] User personas (who)
- [ ] User stories with acceptance criteria (what)
- [ ] Success metrics (how we measure)
- [ ] Scope boundaries (what's in/out)

Should have:
- [ ] Business context (why now, why important)
- [ ] Non-functional requirements (performance, security)
- [ ] Example scenarios/use cases

Avoid:
- [ ] ‚ùå Specific technologies or frameworks
- [ ] ‚ùå Code structure or architecture
- [ ] ‚ùå Database schema details
- [ ] ‚ùå API endpoint definitions

**Rule of thumb**: If you're describing *how* to implement, you're too detailed. Focus on *what* the user experiences.

---

### Can I use user stories instead of a PRD?

**Yes**, if your user stories are comprehensive.

**Minimum required** (even with just user stories):

**Must include**:
1. **User stories** with acceptance criteria
2. **Success metrics** (how to measure)
3. **Business context** (why this matters)
4. **Scope** (what's in v1, what's out)

**Good user story format**:
```
As a [user type],
I want [capability],
So that [benefit].

Acceptance Criteria:
- [Specific, testable criterion 1]
- [Specific, testable criterion 2]
- [Specific, testable criterion 3]

Success Metric: [How we measure if this solved the problem]
```

**Example - Good**:
```
As a data analyst,
I want to export dashboard data to CSV,
So that I can perform custom analysis in Excel.

Acceptance Criteria:
- Export button visible on all dashboard views
- CSV includes all visible columns with proper headers
- File downloads with format: dashboard-{name}-{date}.csv
- Supports datasets up to 10,000 rows
- Shows progress indicator for exports >1000 rows

Success Metric: 30% of analysts export data weekly within 30 days
```

**Example - Bad**:
```
As a user,
I want exports,
So that I can use data.
```

**Bottom line**: Format doesn't matter. Clarity and completeness do.

---

### What if requirements change mid-development?

**This is normal.** The workflow handles changes gracefully.

**Small changes** (acceptance criteria adjustments):
1. Update the PRD
2. Tell AI what changed
3. AI updates implementation
4. Re-run testing and review
5. Time impact: 30-60 minutes

**Medium changes** (new acceptance criteria added):
1. Update PRD and ticket
2. Re-run technical planning for new tickets
3. AI implements new criteria
4. Re-run full quality gates
5. Time impact: 2-4 hours

**Large changes** (scope change, different feature):
1. Stop current work
2. Write new PRD
3. Run full workflow from scratch
4. Time impact: Start over, but still faster than traditional development

**Example scenario**:
- Started with CSV export feature
- Customer requested Excel format too
- Decision: Added Excel to scope
- AI implementation required additional time
- Still faster than restarting from scratch manually

**Best practice**:
- **Small changes**: Go ahead and adjust
- **Medium changes**: Evaluate if it's worth delay
- **Large changes**: Consider if this should be a separate feature

---

### How do I write good acceptance criteria?

**Good acceptance criteria are**:
- ‚úÖ Specific and testable
- ‚úÖ Focused on user behavior, not implementation
- ‚úÖ Independent (can be tested separately)
- ‚úÖ Measurable or observable

**Format**: Given/When/Then or checkbox list

**Example 1: Given/When/Then**
```
Given a user with export permissions is viewing a dashboard,
When they click the "Export to CSV" button,
Then a CSV file downloads with:
  - All visible columns from the dashboard
  - Column headers matching dashboard labels
  - File name format: dashboard-{name}-{date}.csv
  - UTF-8 encoding for special characters
```

**Example 2: Checkbox List**
```
Export Functionality:
- [ ] Export button visible on all dashboard pages
- [ ] Button only visible to users with 'export' permission
- [ ] Clicking button initiates CSV download
- [ ] CSV includes all visible columns
- [ ] CSV respects current filters and date range
- [ ] File name includes dashboard name and date
- [ ] Loading indicator appears for exports >1000 rows
- [ ] User can cancel long-running exports
```

**Bad acceptance criteria** ‚ùå:
```
- CSV export works
- It should be fast
- Users can export data
- The feature is good quality
```
*Why bad*: Not specific, not testable, not measurable.

**Good acceptance criteria** ‚úÖ:
```
- Export completes in <5 seconds for 5,000 rows (p95)
- Export button has aria-label for screen readers
- Export respects user's data permissions
- Error message appears if export fails with retry option
- User receives confirmation when export completes
```
*Why good*: Specific, testable, measurable.

---

## Security & Compliance

### Is AI-generated code secure?

**It can be, with proper review.** The security review phase is critical.

**Built-in security**:
1. **AI trained on security best practices** (OWASP, CVE databases)
2. **Security review phase** catches 95%+ of issues before production
3. **Pattern matching** ensures new code follows secure existing patterns
4. **Automated testing** includes security test cases

**What security review catches**:
- üîí SQL injection vulnerabilities
- üîí XSS (cross-site scripting) issues
- üîí Authentication bypass risks
- üîí Data exposure risks
- üîí Permission/authorization holes

**Your responsibility**:
1. ‚úÖ Never skip security review
2. ‚úÖ Fix all CRITICAL and HIGH severity issues before merge
3. ‚úÖ Review security report even if you don't understand details
4. ‚úÖ Ask questions if something seems risky

**Red flag behavior** üö©:
- Skipping security review to ship faster
- Ignoring CRITICAL findings ("we'll fix later")
- Disabling security checks ("just for now")
- Storing passwords or secrets in code

**Bottom line**: AI code + AI security review = a reasonable security posture. 

---

### Can I use this for HIPAA/SOC2 compliance?

**Yes, but with extra care.**

**HIPAA/SOC2 requirements**:
- ‚úÖ Code must be reviewed (security review does this)
- ‚úÖ Access must be logged (audit logging covered)
- ‚úÖ Data must be encrypted (AI follows existing patterns)
- ‚úÖ Changes must be tracked (Git provides this)

**Extra steps for compliance**:

**1. Enhanced Security Review**
- Run security review for EVERY change (no exceptions)
- Add compliance-specific checks to review
- Document all security decisions

**2. Audit Trail**
- Keep all tickets and PR descriptions
- Document who approved what and when
- Track all access to production data

**3. Human Review Required**
- Have a senior engineer review AI-generated code
- Document engineer sign-off
- Compliance auditors want human accountability

**4. Testing Requirements**
- Test data privacy scenarios explicitly
- Verify data isolation between users/tenants
- Test encryption at rest and in transit

**Real example**: Example 4 (Multi-Tenant Dashboard) was HIPAA-compliant healthcare SaaS
- Followed all steps above
- Security review caught 2 tenant isolation issues
- Senior engineer reviewed all changes
- Passed HIPAA audit with zero findings

**Bottom line**: This workflow can support compliance, but you need extra human oversight.

---

### Who owns the code generated by AI?

**You do.** Code ownership is the same as human-written code.

**Legal perspective**:
- AI-generated code is owned by you/your company
- Same as if you hired a contractor to write code
- No special licensing restrictions (for most AI tools)

**Claude Code specifically**:
- You own all code you generate with Claude Code
- Anthropic doesn't claim ownership
- Standard commercial use permitted

**Best practices**:
1. **Review terms of service** for your specific AI tool
2. **Check with legal** if you're in a regulated industry
3. **Document AI usage** in compliance logs if required
4. **Treat AI code like any other code** in your repository

**Copyright considerations**:
- AI doesn't copy code verbatim from training data
- Generated code is original (based on patterns, not copying)
- Still run plagiarism checks if you're paranoid

---

### What about secrets and API keys?

**Never put secrets in PRDs or prompts.**

**Safe approach**:

**In PRDs** ‚úÖ:
```
The system should connect to the email service using credentials
stored in environment variables.
```

**NOT in PRDs** ‚ùå:
```
Use API key: sk_live_abc123xyz...
Connect to SMTP: user@example.com password: MyPassword123
```

**AI handling of secrets**:
- AI will use environment variables (not hardcoded secrets)
- AI will use secure secret management (AWS Secrets Manager, etc.)
- AI will follow your existing patterns for secrets

**Verification**:
- Security review flags any hardcoded secrets
- Code review checks for credential exposure
- Git hooks prevent committing .env files

**Real example**:
Security review caught this in Example 3:
```javascript
// AI initially generated:
const apiKey = process.env.SENDGRID_API_KEY;

// Security review flagged:
‚ö†Ô∏è No fallback handling if environment variable missing

// AI fixed:
const apiKey = process.env.SENDGRID_API_KEY;
if (!apiKey) {
  throw new Error('SENDGRID_API_KEY environment variable required');
}
```

---

## Team & Organization

### How do I get my engineers to trust this?

**Start with transparency and proof.**

**Week 1: Show, Don't Tell**
1. Pick a small, non-critical feature
2. Run through the full workflow
3. Invite engineers to review the PR
4. Show them test coverage, security report, code quality

**Week 2: Address Concerns**

Common engineer concerns and how to address:

**"AI code is bad quality"**
‚Üí Show them the test coverage (90%+) and code review results

**"AI doesn't understand our patterns"**
‚Üí Show them service inventory and discovery findings

**"This will replace us"**
‚Üí Explain this frees them for complex work they actually enjoy

**"I don't trust black box code"**
‚Üí Show transparency: they can review everything, quality gates are thorough

**Week 3: Collaborate**
- Have engineer review AI-generated PR as they would human PR
- Incorporate their feedback
- Show AI can iterate based on engineer input

**Week 4: Measure**
- Show velocity improvement
- Show quality metrics (bugs, test coverage)
- Show time engineers saved

**Real example**:
One team had a skeptical senior engineer:
- Week 1: "This is garbage"
- Week 2: "Okay, the tests are actually good"
- Week 3: "This follows our patterns better than our junior devs"
- Week 4: "Can we use this for all CRUD features?"

**Key success factors**:
1. **Don't force it** - make it optional initially
2. **Show metrics** - data beats opinions
3. **Respect expertise** - engineers review and approve
4. **Share wins** - celebrate time saved and quality achieved

---

### Can multiple PMs use this on the same codebase?

**Yes**, with coordination.

**Best practices**:

**1. Shared Service Inventory**
- Run service inventory weekly (automated)
- Share inventory file with all PMs
- Update after each feature ships

**2. Communication**
- Daily standup or Slack channel for coordination
- Flag when working in same code areas
- Share discovery findings

**3. Git Workflow**
- Each PM works on separate feature branch
- Merge to main frequently (avoid long-lived branches)
- Run service inventory after each merge

**4. Ticketing System Organization**
- Use projects/boards to separate PM ownership
- Use labels for different teams/areas
- Use team assignments for tickets

**Common conflict scenarios**:

**Scenario 1: Two PMs editing same service**
```
PM A: Adding CSV export using ReportService
PM B: Adding PDF export using ReportService

Solution:
- Run service inventory before starting
- Coordinate: PM A goes first (smaller change)
- PM B reruns discovery after PM A merges
- AI adapts to PM A's changes automatically
```

**Scenario 2: Duplicate functionality**
```
PM A: Building user profile photos
PM B: Building team avatar images

Solution:
- Both use service inventory
- Discover overlap
- Agree to share image upload service
- One PM builds service, other reuses
```

**Real example**:
Company with 3 PMs using this workflow:
- Shared service inventory updated daily (automated)
- Weekly PM sync to discuss upcoming features
- Result: 78% code reuse, minimal conflicts

---

### What if I have offshore/distributed teams?

**This workflow works great for distributed teams.**

**Advantages for distributed**:
1. **Async by default** - AI works while you sleep
2. **Documentation built-in** - Everything in tickets
3. **Fewer meetings** - Requirements are clear in PRDs
4. **Time zone coverage** - AI works 24/7

**Best practices**:

**1. Time Zone Handoffs**
```
PM (US): Writes PRD, runs epic planning (morning)
‚Üì AI works during US afternoon
Engineer (India): Reviews PR (their morning = US evening)
‚Üì AI fixes issues during India afternoon
PM (US): Approves and merges (their morning)
```

**2. Clear Documentation**
- PRDs are extra detailed for offshore context
- All decisions documented in tickets
- Video walkthroughs for complex requirements
- Acceptance criteria very explicit

**3. Automated Handoffs**
- Use ticketing system automations for status updates
- Slack notifications for PR ready to review
- Automated daily summary reports

**Real offshore workflow**:
```
Day 1 Morning (US):
- PM writes PRD
- Runs service inventory + discovery + epic planning
- Reviews and approves epics
- Hands off to AI for implementation

Day 1 Evening (US) / Day 2 Morning (India):
- AI completes implementation
- Indian engineer reviews PR
- Provides feedback in PR comments

Day 2 Afternoon (India) / Day 2 Morning (US):
- AI incorporates feedback
- Reruns tests and quality gates
- PM reviews and approves

Day 2 Evening (US):
- Merges to production
```

**Total time: 1.5 days** despite 12-hour time difference.

**Communication tips**:
- Over-document vs. Slack conversations
- Record video walkthrough of features
- Use ticket comments for all discussions (async-friendly)
- Weekly sync call to address blockers

---

### How do I train my team on this workflow?

**4-week training program**:

**Week 1: Foundations**
- Read PM_GUIDE.md
- Review EXAMPLES.md case studies
- Watch demo (create one with screen recording)
- Set up tools (Claude Code, ticketing system MCP)

**Week 2: Hands-On Practice**
- Each person picks a small feature
- Work through full workflow with mentor present
- Review results together
- Discuss what went well/poorly

**Week 3: Independent Work**
- Each person does a feature independently
- Mentor reviews approach and results
- Group debrief on learnings
- Start building team playbook

**Week 4: Optimization**
- Review metrics from all features
- Identify common patterns and shortcuts
- Create team-specific templates
- Establish team norms

**Training materials to create**:

**1. Internal Wiki**
- Workflow overview (link to PM_GUIDE.md)
- Your company's specific adaptations
- PRD templates for your domain
- Links to successful examples from your team

**2. Video Tutorials**
- 10-min: Workflow overview
- 15-min: Writing AI-friendly PRDs
- 20-min: Running through full workflow
- 10-min: Reviewing quality gates

**3. Templates**
- PRD template with placeholders
- Acceptance criteria examples
- Ticket templates
- Review checklists

**Ongoing training**:
- Monthly "wins" showcase (team shares successful features)
- Bi-weekly office hours (Q&A for team)
- Shared Slack channel for tips and questions
- Documentation of lessons learned

**Metrics to track during training**:
- Time from PRD to production (should decrease)
- Quality metrics (should stay high or improve)
- PM confidence (survey monthly)
- Feature complexity (should increase over time)

**Expected progression**:
- Month 1: Simple CRUD features, 70% longer than expected
- Month 2: Medium features, matching benchmarks
- Month 3: Complex features, faster than benchmarks
- Month 4+: Team at full productivity

---

## Troubleshooting

### The AI created duplicate epics. How do I fix this?

**Prevention** (Step 0 in epic-planning):
The workflow has built-in duplicate prevention. Make sure you:
1. ‚úÖ Run epic-planning command (includes pre-flight check)
2. ‚úÖ Review existing epics manifest
3. ‚úÖ Approve execution plan before creation

**If duplicates already exist**:

**Option 1: Delete and Recreate** (recommended for fresh starts)
```bash
# In your ticketing system, delete the duplicate epics
# Re-run epic-planning with clean state
/epic-planning prd.md discovery.md "context"
# This time, carefully review the existing epics manifest
```

**Option 2: Merge Duplicates** (if work has started)
```bash
# In your ticketing system:
1. Pick the "primary" epic to keep
2. Move sub-tickets from duplicate to primary
3. Archive or delete duplicate epic
4. Update epic description to merge requirements

# Update your local tracking
# Re-run service inventory to reflect current state
```

**Option 3: Update Existing** (if new epic adds capabilities)
```bash
# If new "duplicate" actually extends existing:
1. Update existing epic description with new requirements
2. Delete the duplicate epic
3. Run /planning on updated epic to create new sub-tickets
```

**Why duplicates happen**:
- Skipped the pre-flight check
- Epic titles are similar but requirements differ
- Service inventory was outdated

**Lessons learned**:
- Always review existing epics manifest before approving
- Use unique, descriptive epic titles
- Keep service inventory updated after each feature

---

### Tests are failing. What do I do?

**Step 1: Understand the failure**

Look at test output:
```
‚ùå FAILED: should allow export for users with permissions

Expected: true
Received: false

at line 42: expect(canExport).toBe(true)
```

**Step 2: Categorize the failure**

**Category 1: Test is Correct, Code is Wrong** (90% of cases)
- Test validates acceptance criteria correctly
- Code doesn't meet criteria
- **Fix**: Tell AI "The export permission check isn't working. Users with 'export' permission should be able to export."

**Category 2: Test is Wrong, Code is Correct** (8% of cases)
- Code correctly implements requirement
- Test checks for wrong thing
- **Fix**: Tell AI "The test expects X but the requirement is Y. Please fix the test."

**Category 3: Both Wrong** (2% of cases)
- Requirements were unclear
- AI misunderstood
- **Fix**: Clarify acceptance criteria, ask AI to fix both

**Step 3: Let AI fix**

Don't try to fix tests yourself. Instead:

**Good feedback** ‚úÖ:
```
The export functionality should only be available to users with
the 'export' permission. Currently, the test is failing because
users without permission can export. Please fix the permission
check and ensure the test passes.
```

**Bad feedback** ‚ùå:
```
Line 42 in export.service.ts should check user.permissions.includes('export')
before allowing export
```

**Step 4: Re-run quality gates**

After AI fixes:
1. Re-run tests (should pass now)
2. Re-run code review (verify fix is clean)
3. Re-run security review (if permission-related)

**If tests still fail** after 2 attempts:
- Review acceptance criteria (might be ambiguous)
- Provide specific example scenario
- Consider if requirement is actually correct

**Real example**:
```
Initial failure:
‚ùå should export 10,000 rows
Expected rows: 10,000
Actual rows: 5,000

PM feedback:
"The export is limited to 5,000 rows but should support 10,000.
Please increase the limit to 10,000 and add pagination if needed."

AI fix:
- Increased row limit to 10,000
- Added chunked processing for large exports
- Updated test to verify 10,000 rows

Result: ‚úÖ All tests passing
```

---

### Security review found critical issues. Now what?

**üö® STOP: Do not merge or deploy.**

**Step 1: Read the security report**

Look for CRITICAL items:
```
üö® CRITICAL - SQL Injection Vulnerability
Location: src/services/report.service.ts:45
Risk: User input directly in SQL query allows data access
CVSS: 9.1 (Critical)

Vulnerable code:
const results = await db.query(`SELECT * FROM reports WHERE id = ${userId}`)

Recommended fix:
Use parameterized queries:
const results = await db.query('SELECT * FROM reports WHERE id = $1', [userId])
```

**Step 2: Understand the risk** (non-technical)

Security report will explain:
- **What's vulnerable**: Which part of the code
- **What can happen**: What an attacker could do
- **How to fix**: What needs to change

**Example translations**:

**SQL Injection** = Attacker can read/modify/delete any data in database
**XSS** = Attacker can run malicious code in user's browser
**Auth Bypass** = Attacker can access accounts without logging in
**Data Exposure** = Sensitive data visible to unauthorized users

**Step 3: Tell AI to fix**

You don't need to know HOW to fix, just THAT it needs fixing:

**Good request** ‚úÖ:
```
Security review found a critical SQL injection vulnerability.
Please fix all SQL injection issues and re-run security review.
```

**Even better** ‚úÖ:
```
Security review found SQL injection in the report service.
This allows unauthorized data access. Please use parameterized
queries as recommended and ensure all user input is properly
escaped throughout the codebase.
```

**Step 4: Verify fix**

After AI fixes:
1. Re-run security review (should show no critical issues)
2. Verify tests still pass
3. Review the specific lines that were flagged
4. Check if similar issues exist elsewhere

**Step 5: Document**

In ticket comments:
```
Security Issue Log:
- CRITICAL SQL injection found in report service
- Fixed: Changed to parameterized queries
- Verified: Security review re-run shows no issues
- Prevention: Added SQL injection tests to test suite
```

**What if AI can't fix it?**

Rare, but possible:
1. Ask AI to explain what's difficult
2. Consider if requirement needs to change
3. Escalate to senior engineer if truly stuck
4. Document in tech debt backlog

**Real example**:
Example 3 (Notifications) security review:
```
üö® CRITICAL: WebSocket authentication bypass
Risk: Unauthenticated users can subscribe to any notification channel

AI fix:
- Added JWT authentication to WebSocket handshake
- Implemented per-channel authorization checks
- Added session validation on every message
- Re-ran security review: ‚úÖ No critical issues

Time to fix: 45 minutes
```

**Bottom line**: CRITICAL issues are blocking. Don't ship until fixed. This is non-negotiable.

---

### The implementation doesn't match my PRD. Help!

**First, verify it actually doesn't match**:

Sometimes AI interprets requirements better than you wrote them.

**Checklist**:
- [ ] Are all acceptance criteria met? (check PR description)
- [ ] Does it solve the user problem? (even if differently than expected)
- [ ] Are success metrics achievable?
- [ ] Is the UX better or worse than imagined?

**If truly doesn't match**:

**Step 1: Identify the gap**

**Be specific**:
```
‚úÖ Good:
"The PRD says export button should be on every dashboard page.
Currently it's only on the main dashboard. Please add it to
the sub-dashboards as well."

‚ùå Vague:
"This isn't what I wanted."
```

**Step 2: Check if it's an AI misunderstanding or unclear PRD**

**AI misunderstanding** (10% of cases):
- PRD was clear, AI interpreted differently
- **Fix**: Point out the specific requirement and ask AI to adjust

**Unclear PRD** (90% of cases):
- PRD was ambiguous or incomplete
- AI made reasonable assumption
- **Fix**: Clarify requirement and ask AI to update

**Step 3: Request changes**

**Format**:
```
The current implementation [describe what it does].
However, the requirement is [describe what should happen].

Specific changes needed:
1. [Change 1]
2. [Change 2]
3. [Change 3]

Acceptance criteria to verify:
- [ ] [Criterion 1]
- [ ] [Criterion 2]
```

**Step 4: AI updates**

AI will:
1. Update implementation
2. Update tests
3. Re-run quality gates
4. Update PR description

Time: Usually 30-90 minutes

**Step 5: Verify changes**

- Review updated PR description
- Check that changes match your request
- Run through manual testing
- Approve if satisfied

**Real example**:

**Initial implementation**: Export button in toolbar
**PM expectation**: Export button in each dashboard card
**PM feedback**:
```
The export button is in the main toolbar, but users expect it
on each individual dashboard card (like the refresh button).

Please move the export button to each dashboard card's action menu.
This matches the pattern we use for refresh and share.
```

**AI response**:
- Moved button to dashboard cards
- Added to same action menu as refresh/share
- Updated tests for new location
- Time: 35 minutes

**Preventive measures**:

For your next PRD:
1. Add UI mockups or wireframes
2. Reference existing patterns ("like the share button")
3. Include negative examples ("NOT like...")
4. Be more explicit about placement/UX

---

Got more questions? Open an issue on GitHub or check the [PM_GUIDE.md](PM_GUIDE.md) for more details.
